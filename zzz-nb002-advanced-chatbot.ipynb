{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d733e1a-fff5-4739-b702-cc4af0a69d41",
   "metadata": {},
   "source": [
    "# How to build an advanced Chatbot with session memory using LangChain\n",
    "* Advanced Chatbot LLM App.\n",
    "    * Will be able to have a conversation.\n",
    "    * Will remember previous interactions: will have memory.\n",
    "    * Will be able to have different memories for different user sessions.\n",
    "    * Will be able to remember a limited number of messages: limited memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0399355-dece-4701-9bf4-4c204fe74929",
   "metadata": {},
   "source": [
    "## Concepts included\n",
    "* Chat Model vs. LLM Model:\n",
    "    *  Chat Model is based around messages.\n",
    "    *  LLM Model is based around raw text.\n",
    "* Chat History: allows Chat Model to remember previous interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67075f4a-6c25-41db-b851-51df80ffd927",
   "metadata": {},
   "source": [
    "## Trick to avoid the nasty deprecation warnings from LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49577308-4ed9-4c4b-a59a-a825b33ea75d",
   "metadata": {},
   "source": [
    "In this exercise we will use the LangChain legacy chain LLMChain. It works well, but LangChain displays a nasty deprecation warning. To avoid it, we will enter the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "157958ab-f806-4ced-9a9a-bd22cf61e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from langchain._api import LangChainDeprecationWarning\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=LangChainDeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9bf0fd-0661-4350-8979-53266845c8dd",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b08934-b7f0-4e8a-b5f7-9df3c8bdbc66",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bac4cec3-cc60-49f1-a24e-793b3be7cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "992ec4a9-aa01-4e44-aeb9-b9a1f3aa9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "chat = ChatGroq(\n",
    "    groq_api_key = os.getenv(\"GROQ_API_KEY\"),\n",
    "    model_name = \"llama3-70b-8192\",\n",
    ")\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Project Demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ace676",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c01d18b-f9f0-427b-a9dc-3d1885160578",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff5ddc8-1f67-4efc-b26f-5a7bfb8fda5c",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed746499-d1b8-41e5-b131-270cf5fa229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2af3ef-c2c7-445f-92bd-a29c68abce25",
   "metadata": {},
   "source": [
    "## Connect with an LLM and start a conversation with it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e646540d-6dae-468c-b5e0-5348106d034b",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa7337f-3d60-4ede-bdf8-aa7a5cffec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce3551e-95ca-41a1-8810-89c495bf93ab",
   "metadata": {},
   "source": [
    "* For this project, we will use OpenAI's gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9afcbc7d-a816-41e3-925f-850883f5770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatbot = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a1d409-7f2f-40a4-90c5-ad77dad3edce",
   "metadata": {},
   "source": [
    "* Human Message: the user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5dc0613-fb6d-4c82-a614-9e7307714303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "messagesToTheChatbot = [\n",
    "    HumanMessage(content=\"My favourite color is blue.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479b36ec-341a-47bc-af32-30cfb939810d",
   "metadata": {},
   "source": [
    "#### Call the ChatModel (the LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db815e32-8e60-46b3-8cce-fbf40e378397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Blue is a wonderful color! It's calming, soothing, and often associated with feelings of trust and reliability. Did you know that blue is also the most common favorite color among people? It's no wonder, since blue is the color of the sky and the ocean, which are two of the most beautiful natural wonders of our world!\\n\\nDo you have a specific shade of blue that you're particularly fond of? Like, maybe a bright cobalt blue or a softer, more muted blue like sky blue?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 102, 'prompt_tokens': 16, 'total_tokens': 118, 'completion_time': 0.395609371, 'prompt_time': 0.000241866, 'queue_time': 0.055265224, 'total_time': 0.395851237}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--be590912-1473-42e3-87e2-b82b80a38a3d-0', usage_metadata={'input_tokens': 16, 'output_tokens': 102, 'total_tokens': 118})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(messagesToTheChatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913fc8c-254f-410d-aa8f-35eb0898855e",
   "metadata": {},
   "source": [
    "#### Track the operation in LangSmith\n",
    "* [Open LangSmith here](smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e62b67b-9798-4705-8b8a-dbfdf8c93ed8",
   "metadata": {},
   "source": [
    "## Check if the Chatbot remembers your favorite color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0ead4ee-bda3-4c52-9bdb-7bf234733055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm just a language model, I don't have have access to personal information about you, so I don't know what your favorite color is.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 16, 'total_tokens': 47, 'completion_time': 0.105904729, 'prompt_time': 0.000215647, 'queue_time': 0.052193632000000004, 'total_time': 0.106120376}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--c944e8c3-e252-490b-a961-00d11d866bd4-0', usage_metadata={'input_tokens': 16, 'output_tokens': 31, 'total_tokens': 47})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke([\n",
    "    HumanMessage(content=\"What is my favorite color?\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec947e-b9b7-43e3-ac67-60027e049f3c",
   "metadata": {},
   "source": [
    "* As you can see, our Chatbot cannot remember our previous interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df0143e-8bc8-45ad-ac6f-05aaf659c683",
   "metadata": {},
   "source": [
    "## Let's add memory to our Chatbot\n",
    "* We will use the ChatMessageHistory package.\n",
    "* We will save the Chatbot memory in a python dictionary called chatbotMemory.\n",
    "* We will define the get_session_history function to create a session_id for each conversation.\n",
    "* We will use the built-in runnable RunnableWithMesageHistory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2680fc-8b7a-429f-a570-3c3dc4681037",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23533a37-6060-4d32-9b0c-36a9ea57a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95c1d395-0656-46f4-a14e-70cd3e30e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "\n",
    "chatbotMemory = {}\n",
    "\n",
    "# input: session_id, output: chatbotMemory[session_id]\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in chatbotMemory:\n",
    "        chatbotMemory[session_id] = ChatMessageHistory()\n",
    "    return chatbotMemory[session_id]\n",
    "\n",
    "\n",
    "chatbot_with_message_history = RunnableWithMessageHistory(\n",
    "    chat, \n",
    "    get_session_history\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441ef148-1c37-4006-bb02-780d994803bd",
   "metadata": {},
   "source": [
    "#### What is BaseChatMessageHistory and what it does?\n",
    "BaseChatMessageHistory is what is called an **abstract base class** in Python. [See more info about this here](https://api.python.langchain.com/en/latest/chat_history/langchain_core.chat_history.BaseChatMessageHistory.html). This means it serves as a template or a foundational **blueprint for other classes**. It outlines a set of methods and structures that any class inheriting from it must implement or adhere to, but it cannot be used to create objects directly.\n",
    "\n",
    "Here's a simpler breakdown of what it means for `BaseChatMessageHistory` to be an abstract base class:\n",
    "\n",
    "1. **Blueprint for Other Classes:** It provides a predefined structure that other classes can follow. Think of it like an outline or a checklist for building something; it specifies what needs to be included, but it isn’t the final product.\n",
    "\n",
    "2. **Cannot Create Instances:** You cannot create an instance of an abstract base class. Trying to create an object directly from `BaseChatMessageHistory` would result in an error because it's meant to be a guide, not something to use directly.\n",
    "\n",
    "3. **Requires Implementation:** Any class that inherits from this abstract base class needs to implement specific methods outlined in `BaseChatMessageHistory`, such as methods for adding messages, retrieving messages, and clearing messages. The class sets the rules, and the subclasses need to follow these rules by providing the actual operational details.\n",
    "\n",
    "4. **Purpose in Design:** Using an abstract base class helps ensure consistency and correctness in the implementation of classes that extend it. It's a way to enforce certain functionalities in any subclass, making sure that they all behave as expected without rewriting the same code multiple times.\n",
    "\n",
    "Overall, the concept of an abstract base class is about setting standards and rules, while leaving the specific details of execution to be defined by subclasses that inherit from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c9c5e",
   "metadata": {},
   "source": [
    "#### Let's explain the previous code in simple terms\n",
    "The previous code manages the chatbot's memory of conversations based on session identifiers. Here’s a breakdown of what the different components do:\n",
    "\n",
    "1. **chatbotMemory**:\n",
    "    - `chatbotMemory = {}`: This initializes an empty dictionary where session IDs and their respective chat histories will be stored.\n",
    "\n",
    "2. **get_session_history Function**:\n",
    "    - This function, `get_session_history`, takes a `session_id` as an argument and returns the chat history associated with that session.\n",
    "    - If a chat history for the given `session_id` does not exist in `chatbotMemory`, a new instance of `ChatMessageHistory` is created and assigned to that `session_id` in the dictionary.\n",
    "    - The function ensures that each session has its own unique chat history, stored and retrieved using the session ID.\n",
    "\n",
    "3. **chatbot_with_message_history**:\n",
    "    - `chatbot_with_message_history = RunnableWithMessageHistory(chatbot, get_session_history)`: This line creates an instance of `RunnableWithMessageHistory` using two arguments: `chatbot` and `get_session_history`.\n",
    "    - The `chatbot` is passed along with the `get_session_history` function. This setup integrates the chatbot with the functionality to handle session-specific chat histories, allowing the chatbot to maintain continuity and context in conversations across different sessions.\n",
    "    - **Learn more about RunnableWithMessageHistory** [here](https://python.langchain.com/v0.1/docs/expression_language/how_to/message_history/).\n",
    "\n",
    "Overall, the code organizes and manages a chatbot's memory, enabling it to handle different sessions with users effectively by remembering previous messages within each session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98864e16-66bb-49fb-a72f-2ceb5523b361",
   "metadata": {},
   "source": [
    "#### RunnableWithMessageHistory\n",
    "**When invoking a new RunnableWithMessageHistory, we specify the corresponding chat history using a configurable parameter**. Let's say we want to create a chat memory for one user session, let's call it session1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d51d451-ed97-4752-88df-de8072e45f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "session1 = {\"configurable\": {\"session_id\": \"001\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6d3961a-b565-45b5-a45f-2332ab03cd87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I remember! You told me earlier that your favorite color is red!'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My favorite color is red.\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "378a4aae-c392-439d-a7ea-4ae91a677075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Easy one! Your favorite color is RED!'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my favorite color?\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d37c98-3883-4210-a162-5302e109e743",
   "metadata": {},
   "source": [
    "## Let's now change the session_id and see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b484c1-da0b-4e90-a304-afbbebe63b76",
   "metadata": {},
   "source": [
    "Now let's create a chat memory for another user session, let's call it session2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f82b3e0-7897-4aa8-b554-1985a3af4a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "session2 = {\"configurable\": {\"session_id\": \"002\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75248e78-2662-4faa-bd41-4aa2b7963d19",
   "metadata": {},
   "source": [
    "If the chatbot is using this new memory for session2, it will not be able to remember anything from the previous conversation in the session1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "882d0fc2-4360-4205-8cc7-bc7275295eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I still don't know! You haven't shared your favorite color with me yet!\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my favorite color?\")],\n",
    "    config=session2,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0ec598-2b9f-4da1-b169-b270f820df9e",
   "metadata": {},
   "source": [
    "## Let's go back to session1 and see if the memory is still there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f36c102a-1f06-490f-88b7-cb6e6380d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "session1 = {\"configurable\": {\"session_id\": \"001\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35f61f9a-b8d5-4149-97cd-c7b6c8683bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I know this one! Your favorite color is RED!'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my favorite color?\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd5156-977b-42e9-89e9-7797f6895ebc",
   "metadata": {},
   "source": [
    "As we can see, the chatbot is now able to remember the session1 conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f037264c-71aa-47f2-9512-e78c12761eaa",
   "metadata": {},
   "source": [
    "## Our ChatBot has session memory now. Let's check if it remembers the conversation from session2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fac2a638-7968-4475-8cf8-9af8440f3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "session2 = {\"configurable\": {\"session_id\": \"002\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d367cb8f-1109-45ca-8e7a-1b76a096ec86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I remember! Your name is indeed Mahima Shetty!'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My name is Mahima Shetty.\")],\n",
    "    config=session2,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39ac2ecd-9800-467e-a612-9e3264185875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Easy one! Your name is Mahima Shetty!'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name?\")],\n",
    "    config=session2,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c649bdcc-a393-406b-8121-4cd25ae11163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I still don't know! You haven't shared your favorite color with me yet, Mahima Shetty!\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my favorite color?\")],\n",
    "    config=session2,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbce7cd-399d-42f3-be81-bff26506cfbc",
   "metadata": {},
   "source": [
    "## Our chatBot now remembers each of our conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39070b3d-5de9-4751-966e-98b17a4db745",
   "metadata": {},
   "source": [
    "## The importance to manage the Conversation History\n",
    "* The memory of a chatbot is included in the context window of the LLM so, if left unmanaged, can potentially overflow it.\n",
    "* **We are now going to learn how to limit the size of the memory of a chatbot**.\n",
    "* First, let's take a look at what is in the memory of our chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09547883-82fc-480a-8f58-fd7fbc007fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'001': InMemoryChatMessageHistory(messages=[HumanMessage(content='My favorite color is red.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Red is a fantastic choice! Red is a bold, energetic, and attention-grabbing emotions. It's often associated with passion, courage, and excitement. Did you know that red is also a stimulating color that can increase heart rate and energy levels?\\n\\nWhat do you think it is about the color red that resonates with you? Is there a particular shade or tone that you're especially drawn to?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 16, 'total_tokens': 98, 'completion_time': 0.278548933, 'prompt_time': 0.000221976, 'queue_time': 0.053199665, 'total_time': 0.278770909}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--c591b9b1-88e4-4a14-9362-943245fe1502-0', usage_metadata={'input_tokens': 16, 'output_tokens': 82, 'total_tokens': 98}), HumanMessage(content=\"What's my favorite color?\", additional_kwargs={}, response_metadata={}), AIMessage(content='I remember! Your favorite color is RED!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 113, 'total_tokens': 123, 'completion_time': 0.03165878, 'prompt_time': 0.003616041, 'queue_time': 0.054463038, 'total_time': 0.035274821}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--e975da3e-f43a-43f8-9728-e3a9904d3aee-0', usage_metadata={'input_tokens': 113, 'output_tokens': 10, 'total_tokens': 123}), HumanMessage(content=\"What's my favorite color?\", additional_kwargs={}, response_metadata={}), AIMessage(content='I remember! You told me earlier that your favorite color is RED!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 138, 'total_tokens': 153, 'completion_time': 0.052399645, 'prompt_time': 0.005088618, 'queue_time': 0.054352012000000005, 'total_time': 0.057488263}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--22629c87-faab-477e-9849-0e0036f10938-0', usage_metadata={'input_tokens': 138, 'output_tokens': 15, 'total_tokens': 153}), HumanMessage(content='What is my favorite color?', additional_kwargs={}, response_metadata={}), AIMessage(content='I know! Your favorite color is RED!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 168, 'total_tokens': 178, 'completion_time': 0.043861705, 'prompt_time': 0.004926961, 'queue_time': 0.053271878, 'total_time': 0.048788666}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--95e23800-5704-498b-923c-441d0dea45b2-0', usage_metadata={'input_tokens': 168, 'output_tokens': 10, 'total_tokens': 178}), HumanMessage(content='My favorite vehicles are Vespa scooters.', additional_kwargs={}, response_metadata={}), AIMessage(content='Vespa scooters are amazing! They\\'re so stylish and iconic, aren\\'t they? Vespa scooters evoke a sense of freedom and adventure, and they\\'re perfect for cruising around town or exploring scenic routes.\\n\\nWhat is it about Vespa scooters that resonates with you? Is it their sleek design, their ease of use, or something else entirely?\\n\\n(By the way, I\\'ve got a fun fact for you: Did you know that the name \"Vespa\" is a combination of the Italian word \"vespa,\" which means \"wasp,\" and the Italian pronunciation of the Piaggio company\\'s name?)', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 196, 'total_tokens': 324, 'completion_time': 0.426601684, 'prompt_time': 0.005822988, 'queue_time': 0.052457290999999996, 'total_time': 0.432424672}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--0194f004-d564-4e4c-8eae-e97cb1212290-0', usage_metadata={'input_tokens': 196, 'output_tokens': 128, 'total_tokens': 324}), HumanMessage(content='My favorite city is San Francisco.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"San Francisco is an amazing choice! San Francisco is a vibrant, eclectic, and iconic city that has something for everyone. From the Golden Gate Bridge to Alcatraz Island, Fisherman's Wharf, and steep hills with colorful Victorian houses, San Francisco is a treasure trove of sights, sounds, and flavors. And let's not forget the famous cable cars!\\n\\nWhat do you love most about San Francisco? Is it the city's laid-back vibe, its rich history, or something else entirely?\\n\\n(By the way, I've got a fun fact for you: Did you know that San Francisco is home to the oldest Chinatown in North America, with a rich history dating back to the mid-1800s?)\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 146, 'prompt_tokens': 340, 'total_tokens': 486, 'completion_time': 0.417142857, 'prompt_time': 0.010755949, 'queue_time': 0.055411318, 'total_time': 0.427898806}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--7d53d27f-9b88-40d6-b681-4167b27dd13d-0', usage_metadata={'input_tokens': 340, 'output_tokens': 146, 'total_tokens': 486}), HumanMessage(content='what My favorite city?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I apologize for the mistake earlier! You didn't mention your favorite city, and I jumped to San Francisco. Please tell me, what is your favorite city, and what makes it special to you?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 191, 'total_tokens': 232, 'completion_time': 0.1784451, 'prompt_time': 0.005821008, 'queue_time': 0.055319431999999995, 'total_time': 0.184266108}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--69ff61a9-a7a1-4cdb-8c8a-06ddd478da8a-0', usage_metadata={'input_tokens': 191, 'output_tokens': 41, 'total_tokens': 232}), HumanMessage(content='what My favorite city?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"You didn't mention a favorite city earlier. That's perfectly fine! If you don't have a favorite city, we can explore different cities and find one that might interest you.\\n\\nWould you like to learn about a new city or explore a particular type of city (e.g., beach city, mountain town, cultural hub)?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 86, 'total_tokens': 152, 'completion_time': 0.288412231, 'prompt_time': 0.002376386, 'queue_time': 0.055518973, 'total_time': 0.290788617}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--02d50cd0-342d-417a-864b-f67eecc715a7-0', usage_metadata={'input_tokens': 86, 'output_tokens': 66, 'total_tokens': 152}), HumanMessage(content='My favorite color is red.', additional_kwargs={}, response_metadata={}), AIMessage(content='I remember! You told me earlier that your favorite color is red!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 636, 'total_tokens': 651, 'completion_time': 0.070938825, 'prompt_time': 0.020293652, 'queue_time': 0.05566014200000001, 'total_time': 0.091232477}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--558d4efe-3354-457f-833b-1e3442ddd701-0', usage_metadata={'input_tokens': 636, 'output_tokens': 15, 'total_tokens': 651}), HumanMessage(content=\"What's my favorite color?\", additional_kwargs={}, response_metadata={}), AIMessage(content='Easy one! Your favorite color is RED!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 666, 'total_tokens': 676, 'completion_time': 0.062124009, 'prompt_time': 0.021253178, 'queue_time': 0.055565682000000005, 'total_time': 0.083377187}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--3f0be0c0-f4af-49f1-9f18-c73b3f41d64e-0', usage_metadata={'input_tokens': 666, 'output_tokens': 10, 'total_tokens': 676}), HumanMessage(content=\"What's my favorite color?\", additional_kwargs={}, response_metadata={}), AIMessage(content='I know this one! Your favorite color is RED!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 691, 'total_tokens': 703, 'completion_time': 0.063301582, 'prompt_time': 0.021705591, 'queue_time': 0.057155649, 'total_time': 0.085007173}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--4b0d12b6-4e53-41cf-870c-708f6c8d504a-0', usage_metadata={'input_tokens': 691, 'output_tokens': 12, 'total_tokens': 703})]), '002': InMemoryChatMessageHistory(messages=[HumanMessage(content=\"What's my favorite color?\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm just a language model, I don't have the ability to know your personal preferences, including your favorite color. If you want to tell me, I'd be happy to know!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 16, 'total_tokens': 55, 'completion_time': 0.111428571, 'prompt_time': 0.000178477, 'queue_time': 0.056103232999999995, 'total_time': 0.111607048}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--8a8b15e6-97d2-4130-a87b-109a2a6e6f20-0', usage_metadata={'input_tokens': 16, 'output_tokens': 39, 'total_tokens': 55}), HumanMessage(content='My name is Mahima Shetty.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Nice to meet you, Mahima Shetty! I'm still a large language model, I don't have any prior knowledge about you or your preferences. Our conversation just started, and I'm excited to chat with you!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 72, 'total_tokens': 118, 'completion_time': 0.170415077, 'prompt_time': 0.001911279, 'queue_time': 0.05404755, 'total_time': 0.172326356}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--1db55942-5545-4b8e-921d-b7edccf9b12b-0', usage_metadata={'input_tokens': 72, 'output_tokens': 46, 'total_tokens': 118}), HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='I remember! Your name is Mahima Shetty!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 132, 'total_tokens': 144, 'completion_time': 0.049781539, 'prompt_time': 0.003806248, 'queue_time': 0.052266382, 'total_time': 0.053587787}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--d0ed897b-0ead-409b-8770-de43c1f872ec-0', usage_metadata={'input_tokens': 132, 'output_tokens': 12, 'total_tokens': 144}), HumanMessage(content='What is my favorite color?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't know! You never told me!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 159, 'total_tokens': 170, 'completion_time': 0.069771158, 'prompt_time': 0.004683644, 'queue_time': 0.056399655, 'total_time': 0.074454802}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--10fc350e-9698-480f-9b54-dbcca77937b4-0', usage_metadata={'input_tokens': 159, 'output_tokens': 11, 'total_tokens': 170}), HumanMessage(content=\"What's my favorite color?\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"I still don't know! You haven't shared your favorite color with me yet!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 185, 'total_tokens': 203, 'completion_time': 0.063031585, 'prompt_time': 0.005869027, 'queue_time': 0.055930881, 'total_time': 0.068900612}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--273494a9-3dc0-4c2f-9642-2ae0a70d83bd-0', usage_metadata={'input_tokens': 185, 'output_tokens': 18, 'total_tokens': 203}), HumanMessage(content='My name is Mahima Shetty.', additional_kwargs={}, response_metadata={}), AIMessage(content='I remember! Your name is indeed Mahima Shetty!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 220, 'total_tokens': 233, 'completion_time': 0.053623338, 'prompt_time': 0.006598596, 'queue_time': 0.053635174, 'total_time': 0.060221934}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--9a057184-d90e-48ea-b284-04fde0d7713b-0', usage_metadata={'input_tokens': 220, 'output_tokens': 13, 'total_tokens': 233}), HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Easy one! Your name is Mahima Shetty!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 247, 'total_tokens': 259, 'completion_time': 0.051172303, 'prompt_time': 0.01132692, 'queue_time': 0.05721591899999999, 'total_time': 0.062499223}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--a8ad3dba-5d8e-40e5-8b1c-3d034773b32e-0', usage_metadata={'input_tokens': 247, 'output_tokens': 12, 'total_tokens': 259}), HumanMessage(content='What is my favorite color?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I still don't know! You haven't shared your favorite color with me yet, Mahima Shetty!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 274, 'total_tokens': 297, 'completion_time': 0.076956667, 'prompt_time': 0.008678446, 'queue_time': 0.057448464, 'total_time': 0.085635113}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--3a7b087e-75e5-4013-b3ba-bde9d105b0dc-0', usage_metadata={'input_tokens': 274, 'output_tokens': 23, 'total_tokens': 297})])}\n"
     ]
    }
   ],
   "source": [
    "print(chatbotMemory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056a7229-ea29-468b-93e1-b2b52951b47c",
   "metadata": {},
   "source": [
    "* Now, **let's define a function to limit the number of messages stored in memory and add it to our chain with .assign**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd33a1bb-d3c4-4011-99c7-105e3d7051e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def limited_memory_of_messages(messages, number_of_messages_to_keep=2):\n",
    "    return messages[-number_of_messages_to_keep:]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "limitedMemoryChain = (\n",
    "    RunnablePassthrough.assign(messages=lambda x: limited_memory_of_messages(x[\"messages\"]))\n",
    "    | prompt \n",
    "    | chat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff26a10-27aa-48ef-a3f0-867d3f819c79",
   "metadata": {},
   "source": [
    "* The limited_memory_of_messages function allows you to trim the list of stored messages, keeping only a specified number of the latest ones. For example, if you have a long list of messages and you only want to keep the last two, this function will do that for you.\n",
    "* The lambda function works in conjunction with the `limited_memory_of_messages` function. Here’s a simple breakdown:\n",
    "\n",
    "    1. **Lambda Function**: The `lambda` keyword is used to create a small anonymous function in Python. The `lambda` function defined here takes one argument, `x`.\n",
    "\n",
    "    2. **Function Argument**: The argument `x` is expected to be a dictionary that contains a key named `\"messages\"`. The value associated with this key is a list of messages.\n",
    "\n",
    "    3. **Function Body**: The body of the `lambda` function calls the `limited_memory_of_messages` function. It passes the list of messages found in `x[\"messages\"]` to this function.\n",
    "\n",
    "    4. **Default Behavior of limited_memory_of_messages**: Since the `lambda` function does not specify the `number_of_messages_to_keep` parameter when it calls `limited_memory_of_messages`, the latter will default to keeping the last 2 messages from the list (as defined by the earlier function).\n",
    "\n",
    "In essence, the `lambda` function is a shorthand way to apply the `limited_memory_of_messages` function to the message list contained within a dictionary. It automatically trims the list to the last two messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac98d2fd-4e32-4c71-a0c0-99701534f551",
   "metadata": {},
   "source": [
    "**Let's now create our new chatbot with limited message history**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e45d4cd1-2165-451d-939f-66f9cc898c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_with_limited_message_history = RunnableWithMessageHistory(\n",
    "    limitedMemoryChain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b39981-02c7-4d10-a4a8-2efbe9bb3c89",
   "metadata": {},
   "source": [
    "## Let's add 2 more messages to the session1 conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76687e6a-2bbf-450f-8310-7011646fea3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vespa scooters are amazing! They\\'re so stylish and iconic, aren\\'t they? Vespa scooters evoke a sense of freedom and adventure, and they\\'re perfect for cruising around town or exploring scenic routes.\\n\\nWhat is it about Vespa scooters that resonates with you? Is it their sleek design, their ease of use, or something else entirely?\\n\\n(By the way, I\\'ve got a fun fact for you: Did you know that the name \"Vespa\" is a combination of the Italian word \"vespa,\" which means \"wasp,\" and the Italian pronunciation of the Piaggio company\\'s name?)'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My favorite vehicles are Vespa scooters.\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c8a9b0c-7ae5-4f3b-b93b-06462c6d3199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"San Francisco is an amazing choice! San Francisco is a vibrant, eclectic, and iconic city that has something for everyone. From the Golden Gate Bridge to Alcatraz Island, Fisherman's Wharf, and steep hills with colorful Victorian houses, San Francisco is a treasure trove of sights, sounds, and flavors. And let's not forget the famous cable cars!\\n\\nWhat do you love most about San Francisco? Is it the city's laid-back vibe, its rich history, or something else entirely?\\n\\n(By the way, I've got a fun fact for you: Did you know that San Francisco is home to the oldest Chinatown in North America, with a rich history dating back to the mid-1800s?)\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My favorite city is San Francisco.\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c688e6-3e9f-4c11-a4f7-38eb8dd35666",
   "metadata": {},
   "source": [
    "## The chatbot memory has now 4 messages. Let's check the Chatbot with limited memory. \n",
    "* Remember, this chatbot only remembers the last 2 messages, so if we ask her about the first message she should not remember it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16875068-39fd-419e-84b0-f3363f30a3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You didn't mention a favorite city earlier. That's perfectly fine! If you don't have a favorite city, we can explore different cities and find one that might interest you.\\n\\nWould you like to learn about a new city or explore a particular type of city (e.g., beach city, mountain town, cultural hub)?\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_limited_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what My favorite city?\")],\n",
    "    },\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9989ae1-d75b-4b8f-9235-b4eb110e102d",
   "metadata": {},
   "source": [
    "* The chatbot with limited memory has behaved as we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d324091a-dc49-42af-b638-324dc646cdae",
   "metadata": {},
   "source": [
    "## Finally, let's compare the previous response with the one provided by the Chatbot with unlimited memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "251e1915-dabf-4591-82cc-a0fa8b465ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No question about it! Your favorite color is RED!'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"what is my favorite color?\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83e34b2-3c42-4ef7-9deb-18c2ec374adc",
   "metadata": {},
   "source": [
    "* As you can see, this chatbot remembers our first message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f483274a-ad1f-4079-b053-ebdcde113ec0",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 004-invoke-stream-batch.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 002-advanced-chatbot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2ffb53-77c1-4e62-a47c-cf50af2a3634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9409d6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909c41e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
